### Delta Reward
Calculating rewards based only on the current state can result in a large reward magnitude, which may lead to overly significant updates in the agent's weights during reinforcement learning. To mitigate this, the reward for a particular action should be computed based on the difference between the current and previous states, known as the delta reward. This can be represented as:
**R = R(S_(t)) - R(S_(t-1))**

This approach allows rewards to reflect the improvement between the current and previous states:
**reward = current_reward - previous_reward**

If multiple factors influence the reward calculation, calculate the difference for each factor separately, then add them to the total reward. When different weights apply to each factor, calculate the difference, multiply it by the weight, and add it to the reward. For example:

**diff = factor_current_state - factor_previous_state**
**reward = diff * weight**

---

### Dense Reward
**Case 1:**
If the shape created by the reward function is too small, the reward values may become sparse. Small shapes may not be easily recognized by the evaluation function, so to ensure the agent receives prompt rewards, the shape size in the target map should be large enough. To address this, the target shape’s width should be appropriately scaled relative to the map size, and the shape’s thickness should be adjusted to control reward frequency.

** Case 2:**
The shape created by the reward function should have a certain level of thickness, and it is advisable to set this thickness generously. If the shape is too thin (width is similar to 1), the agent will receive rewards infrequently, leading to sparse feedback and slower learning. To ensure consistent rewards and optimal performance, adjust the shape's thickness to be ample enough for the evaluation function to recognize it effectively within the target map.

**Case 3:**
When calculating rewards based on specific conditions, highly strict conditions can result in sparse rewards. For example:
**reward += jax.lax.cond(num_players == target_count, lambda _: reward_value, lambda _: penalty_value, None)**

Instead, make the conditions more flexible for continuous rewards, which encourages more dense rewards. Compute the error between the current and previous states first and then apply delta reward as the reward value. This provides feedback each time the criteria change:
**reward = abs(previous_count - target_count) - abs(current_count - target_count)**

---

### General Reward Function

- The reward function should work across diverse level sizes, considering varying widths and heights.
- To generate various target shapes, avoid creating hard-coded levels.

---

### Format
- Do not use "`", double quotes, and single quotes character in the reward code and notes for the parsing from the response.
- Import the necessary library for the reward function code. (i.e., import jax.numpy as jnp)
- Make sure the empty space of the level is represented as 1, not 0. The unpassible tile (wall) should be represented as 2.