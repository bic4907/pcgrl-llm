# Task
Evaluate the generated content (level) based on the following evaluation criteria and provide scores for each metric.

## Criteria
Each score is measured between 0 and 1, rounded to 2 decimal places:
- **Playability**: How playable the level is based on defined constraints.
- **Path Length**: The average length of successful paths in the level.
- **Solvability**: The likelihood that the level can be completed.
- **Number of Solutions**: The number of unique solutions to the level.
- **Loss Solutions**: The proportion of potential solutions that fail.
- **Reach Important Tile Percentage**: Improvement in reaching critical goals.
- **Existence Important Tile Percentage**: Improvement in ensuring necessary conditions exist.
- **Accuracy Important Tile Percentage**: Improvement in accuracy of intended results.
- **False Positive Improvement Percentage (`fp_imp_perc`)**: Reduction in unintended false positives.
- **False Negative Important Tile Percentage (`fn_imp_perc`)**: Reduction in unintended false negatives.

## Output
Return the similarity and diversity scores as a json string.
Return a JSON string with all metric scores. Provide reasons for the scores and include them in a summary after the JSON output.
If there are multiple content in the "Generated Contents", response with averaged scores for each metric.




### Output Example (JSON)
```
{
  "playability": 0.85,
  "path_length": 22,
  "solvability": 0.90,
  "n_solutions": 0.68,
  "loss_solutions": 0.15,
  "reach_imp_perc": 0.80,
  "exist_imp_perc": 0.92,
  "acc_imp_perc": 0.88,
  "fp_imp_perc": 0.10,
  "fn_imp_perc": 0.07
}
```

** Generated Contents **
Evaluating the following contents which is generated by the model.

{content}

** Generation Goal **
{evaluation_criteria}

**Response**
Output a JSON string with scores for each metric. Include a brief reason for the scores in plain text after the JSON output.
